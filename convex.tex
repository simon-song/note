\chapter{Convex Sets}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Separating Hyperplane}

%%%%%%%%%%%%%%%%%%%
\begin{definition}
A hyperplane $H(a,\alpha)$ is defined as
\[
  H(a,\alpha)=\{x: \langle a,x \rangle = \alpha \}.
\]
\end{definition}

If $a\in R^n$ and $X\subset R^n$, we let
\[
  \langle a,X \rangle = \{ \langle a,x \rangle : x\in X \},
\]
and write 
\[
  \langle a,X \rangle < b
\] 
to denote the fact that $\langle a,x \rangle <b$ for all $x\in X$.

%%%%%%%%%%%%%%%%%%%
\begin{definition}
\footnote{Berkovitz, Convexity and Optimization in $R^n$, Definition 3.4 and
Lemma 3.1, pp.47-48}
Two subsets $X$ and $Y$ of $R^n$ are strongly separated by a hyperplane
$H(a,\alpha)$ if there exists $\epsilon>0$ for which one of the following
holds:
\begin{itemize}
  \item[(1)] $\langle a,X \rangle < \alpha-\epsilon < \alpha+\epsilon
               < \langle a,Y \rangle$
  \item[(2)] $\langle a,Y \rangle < \alpha-\epsilon < \alpha+\epsilon
               < \langle a,Y \rangle$.
\end{itemize}

\end{definition}

%%%%%%%%%%%%%%
\begin{definition}
\footnote{Berkovitz, Convexity and Optimization in $R^n$, p.49}
If $X$ is a set and $y$ is a vector, then we define the distance from $y$
to $X$, denoted by $d(y,X)$, to be 
\[
  d(y,X) = \inf \{ \| y-x \|: x\in X \}.
\]
A point $x_*$ in $X$ is said to attain the distance from $y$ to $X$, or to
be closest to $y$, if $d(y,X)=\| y-x_* \|$.
\end{definition}

%%%%%%%%%%%%%%
\begin{lemma}
\footnote{Adapted from Berkovitz, Convexity and Optimization in $R^n$, Lemma
          3.3, p.50}
Let $C$ be a closed subset of $R^n$ and let $y\notin C$. Then there exists a 
point $x_* \in C$ that is closest to $y$.
\end{lemma}
%%%%%%%%%%%%%%
\begin{proof}
Let $x_0\in C$ and let $r>\| x_0-y \|$. Then $C_1=\overline{B(y,r)}\cap C$ is
nonempty, closed, and bounded and hence is compact. The function
$x\to \| x-y \|$ is continuous on $C_1$ and so attains its minumum at some
point $x_*$ in $C_1$. Thus for all $x\in C_1$, $\| x-y \| \ge \| x_*-y \|$.
For $x\in C\setminus C_1$ we have
\[
  \| x-y \| > r > \| x_0-y \| \ge \| x_*-y \|,
\]
since $x_0 \in \overline{B(y,r)}$.
\end{proof}


%%%%%%%%%%%%%%%
\begin{lemma}
\footnote{Adapted from Berkovitz, Convexity and Optimization in $R^n$, Lemma
          3.2, p.49}
Let $C$ be a convex subset of $R^n$ and let $y\notin C$. If there exists a 
point in $C$ that is closest to $y$, then it is unique.
\end{lemma}
%%%%%%%%%%%%%%
\begin{proof}
Suppose that there were two points $x_1$ and $x_2$ in $C$ that were closest 
to $y$, hence $\| x_1-y \| = \| x_2-y \| = d(y,C)$.

If there exists $k\in R$ such that $x_1-y=k(x_2-y)$, then
\[
  \| x_1-y \| = |k| \| x_2-y \|,
\]
thus $k=0,1,-1$. If $k=0$, then $y=x_1\in C$ which is in contradiction with
$y\notin C$. If $k=1$ then $x_1=x_2$. If $k=-1$ then $y=(x_1+x_2)/2\in C$ 
because $C$ is convex, this again contradicts with $y\notin C$.

Next we consider the case that for every $k\in R$, $x_1-y\neq k(x_2-y)$.
Since $C$ is convex, $(x_1+x_2)/2\in C$, and
\begin{align*}
  \left\| \frac{x_1+x_2}{2} -y \right\|^2 
    &= \frac{1}{4} \left( \| x_1-y \|^2 + \| x_2-y \|^2 
      + 2\langle x_1-y, x_2-y \rangle \right) \notag \\
    &< \frac{1}{4} ( d^2 + d^2 + 2 \| x_1-y \| \| x_2-y ) \|
    = d^2,  \notag
\end{align*}
this contradicts with the definition of $d$ as the distance from $y$ to $C$.
\end{proof}

%%%%%%%%%%%%%%%
\begin{lemma} \label{L:point_close}
\footnote{Adapted from Berkovitz, Convexity and Optimization in $R^n$, Lemma
          3.4, p.50}
Let $C$ be a convex subset of $R^n$ and let $y\notin C$. Then $x_*\in C$ is
a closest point in $C$ to $y$ iff
\[
  \langle y-x_*, x-x_* \rangle \le 0 \qquad \forall x\in C.
\]
\end{lemma}
%%%%%%%%%%%%%%
\begin{proof}
Let $x_*$ be a closest point to $y$ and let $x$ be any point in $C$. Since
$C$ is convex, the line segment
$[x_*,x]=\{z(t):z(t)=x_*+t(x-x_*), 0\le t\le 1\}$
belongs to $C$. Let
\[
  \varphi(t) = \| z(t)-y \|^2 = \langle x_*+t(x-x_*)-y,x_*+t(x-x_*)-y \rangle.
\]
For $0\le t\le 1$, $\varphi(t)$ is the square of the distance between the point
$z(t)\in [x_*,x]$ and $y$. If $t=0$ then $z=x_*$. Since $\varphi$ is 
continuously differentiable on $(0,1]$ and $x_*$ is a point in $C$ closest to
$y$, we have $\varphi'(0+)\ge 0$. Calculating $\varphi'(t)$
\[
  \varphi'(t)=2[-\langle y-x_*,x-x_* \rangle + t\| x-x_* \|^2].
\]
If we let $t\to 0+$ and use $\varphi'(0+)\ge 0$, we get
\[
  \langle y-x_*, x-x_* \rangle \le 0 \qquad \forall x\in C.
\]

To verify the converse, we suppose 
$\langle y-x_*, x-x_* \rangle \le 0$ for all $x\in C$. It follows that 
$\varphi'(t)>0$ for $0<t\le 1$. This $\varphi$ is strictly increasing
function on $[0,1]$, and so for any point $z(t)\in (x_*,x]$ we have
$\| z(t)-y \| > \| x_*-y \|$. In particular $\| x-y \| > \| x_*-y \|$, so
$x_*$ is a closest point to $y$.
\end{proof}


%%%%%%%%%%%%%%%
\begin{theorem} \label{T:sep_plane}
\footnote{Adapted from Berkovitz, Convexity and Optimization in $R^n$, 
          Theorem 3.1, p.49, proof in p.51}
Let $C$ be a closed convex subset of $R^n$ and let $y\notin C$. Then there 
exists a hyperplane $H(a,{\alpha})$ that strongly separates $y$ and $C$.
\end{theorem}
%%%%%%%%%%%%%%
\begin{proof}
Let $x_*$ be the closest point in $C$ to $y$ and let $a=y-x_*$. Then from
Lemma \ref{L:point_close}, we have for all $x\in C$, 
$\langle a,x-x_* \rangle \le 0$, and so 
$\langle a,x \rangle \le \langle a,x_* \rangle$,
with equality occuring when $x=x_*$. Therefore
\[
  \sup\{\langle a,x \rangle : x\in C\} = \langle a,x_* \rangle.
\]
On the other hand,
$\langle a,y-x_* \rangle = \| a \|^2 >0$, so
\[
  \langle a,y \rangle 
    = \langle a,x_* \rangle + \| a \|^2
    > \sup\{\langle a,x \rangle : x\in C\}. 
\]
Hence $y$ and $C$ can be strongly separated.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Farkas's Lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  A^T A has full rank if A has full column rank
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma} \label{L:rank_prod}
\footnote{Adapted from Mirsky, Introduction to Linear Algebra, Theorem 5.5.4,
p.155}
Let $A$ be an $m\times n$ matrix and the columns of $A$ are linearly 
independent, then $(A^T A)_{n\times n}$ is of full rank and thus invertible, 
and for any $y\in R^m$, there exists an $x\in R^n$ such that $y=Ax$. 
\end{lemma}
%%%%%%%%%%%%%%%
\begin{proof}
To show that $(A^T A)_{n\times n}$ has full rank $n$, let $x\in R^n$ satisfying
$(A^T A)x=0$, then
\[
  x^T (A^T A) x = |Ax|^2 = 0,
\]
hence $Ax=0$, thus $x=0$ because the columns of $A$ are linearly independent.
Thus the nullity of $A^T A$ is zero, and it has full rank due to the rank-nullity
theorem \ref{T:rank}.

Because $A^T A$ has full rank, it is invertible, and for any $y\in R^m$,
there exists an $x\in R_n$ such that $y=Ax$, indeed, we have
\[
  x = (A^T A)^{-1} A^T y.
\]
\end{proof}


%%%%%%%%%%%%%%%
\begin{lemma} \label{L:convex_close}
\footnote{Adapted from Berkovitz, Convexity and Optimization in $R^n$, Lemma
          5.1, p.62, and Nocedal and Wright, Numerical Optimization, 2ed,
          Lemma 12.15, p.350}
Let $A$ be an $m\times n$ matrix and 
\[
  C= \{w: w=Ax, w\in R^m, x\in R^n, x\ge 0 \},
\]
then $C$ is a closed convex set.
\end{lemma}
%%%%%%%%%%%%%%%
\begin{proof}
A straight forward calculation verifies that $C$ is a convex cone.

To show that $C$ is closed, we first consider the case that the columns of 
$A$ are linearly independent. Let $w_0$ be a limit point of $C$, and let 
$\{w_k\}$ be a sequence of points in $C$ converging to $w_0$. Then there
exists a sequence of points $\{x_k\}$ in $R^n$ with $x_k\ge 0$ such that
$Ax_k\to w_0$. Using Lemma \ref{L:rank_prod}, we conclude $(A^T A)$
is convertible and 
\[
  x_k = (A^T A)^{-1} A^T w_k.
\]
Also let $x_0 = (A^T A)^{-1} A^T w_0$, and because 
$\lim_{k\to\infty} w_k=w_0$, we have
$\lim_{k\to\infty} x_k=x_0\ge 0$, thus $w_0\in C$ and $C$ is closed. 

If the columns of $A$ are not linearly independent, we first show that any
point $z\in C$ can be expressed as a nonnegative linear combination of $p<n$
linearly independent columns of $A$. Let $A_j, j=1,2,\dots,n$ denote the 
$j$-th column of $A$. Then there exists an $x\ge 0$ such that
\[
  z = \sum_{j=1}^n x_j A_j, \qquad x_j\ge 0.
\]
Since the columns of $A$ are not linearly independent, there exists a
$\mu=(\mu_1,\mu_2,\dots,\mu_n)^T\neq 0$ such that
\[
  0 = \sum_{j=1}^n \mu_j A_j.
\]
Therefore, for any $\rho\in R$,
\[
  z = \sum_{j=1}^n (x_j -\rho \mu_j) A_j 
    = \sum_{\mu_j\neq 0}^n (x_j -\rho \mu_j) A_j 
      + \sum_{\mu_j= 0}^n x_j A_j. 
\]
Set 
\[
  \bar{\rho} =
    \begin{cases}
      \min \left\{ \frac{x_j}{\mu_j}: \mu_j>0 \right\}     &\exists \mu_j>0 \\
      \max \left\{ \frac{x_j}{\mu_j}: \mu_j\neq 0 \right\} &\nexists \mu_j>0 \\
    \end{cases}
\]
then for any $j$ we have $x_j-\bar{\rho}\mu_j\ge 0$ and 
$x_j-\bar{\rho}\mu_j= 0$ for at least one value of $j$. We have now expressed 
$z$ as a nonnegative linear combination of $q<n$ columns of $A$. We continue 
this process until we express $z$ as a nonnegative linear combination of 
$p<n$ linearly independent columns of $A$.

Let $\sigma$ denote a choice of $p<n$ linearly independent columns of $A$ and 
let $A(\sigma)$ denote the corresponding $m\times p$ matrix. There are a finite
number of such choices. Let
\[
  C(\sigma) = \{w: w=A(\sigma) y, w\in R^m, y\in R^p, y\ge 0 \}.
\]
We have shown that each set $C(\sigma)$ is closed and that 
$C\subset \cup_{\sigma} C(\sigma)$.

We now show the opposite inclusion. Let $w\in C(\sigma)$ for some $\sigma$. 
By relabeling columns of $A$, we can assume without loss of generality that
$\sigma$ selets the first $p$ columns of $A$. Then there exists a $y\in R^p$,
$y\ge 0$ such that
\[
  w = \sum_{j=1}^p y_j A_j 
    = \sum_{j=1}^p y_j A_j + \sum_{j=p+1}^n (0) A_j,
\]
i.e. $w\in C$.

Thus $C=\cup_{\sigma} C(\sigma)$, and because all $C(\sigma)$ are closed, we
conclude that $C$ is closed.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Farkas's Lemma] \label{T:farkas}
\footnote{Berkovitz, Convexity and Optimization in $R^n$, Theorem 5.1, 
          pp.63-65; Roman, Advanced Linear Algebra, 3rd ed., Theorem 15.9, 
          p.423}
Let $A$ be an $m\times n$ matrix and let $b$ be a vector in $R^n$. Then one 
and only one of the systems
\begin{itemize}
  \item[(1)] $Ax\le 0$, $\langle b,x \rangle >0$, $x\in R^n$,
  \item[(2)] $A^T y=b$, $y\ge 0$, $y\in R^m$,
\end{itemize}
has a solution.
\end{theorem}
%%%%%%%%%%%%%%%
\begin{proof}
First we suppose that $(2)$ has a solution. If $(1)$ also has a soultion, then
\[
  \langle b,x \rangle = \langle A^T y, x \rangle = \langle y,Ax \rangle \le 0,
\]
because $Ax\le 0$ and $y\ge 0$. This contradicts with the assumption that
$\langle b,x \rangle >0$, thus implies that $(1)$ does not have a solution.

Next we suppose that $(2)$ does not have a solution, let
\[
  C=\{z\in R^n: z=A^T y, y\in R^m, y\ge 0 \},
\]
then $b\notin C$. By Lemma \ref{L:convex_close}, $C$ is convex and closed. 
Therefore, by Theorem \ref{T:sep_plane}, there exists a hyperplane 
$H(x_0,\alpha)$ that strongly separates $b$ and $C$ (hence also strictly 
separates $b$ and $C$). Thus $x_0\neq 0$ and
\[
  \langle x_0,b \rangle > \alpha > \langle x_0,C \rangle.
\]
Since $0\in C$ we get that $\alpha>0$. Thus 
\[
  \langle x_0,b \rangle >0.
\]
For all $z\in C$ we have
\[
  \langle x_0,z \rangle = \langle x_0,A^T y \rangle = y^T Ax_0 < \alpha,
\]
this holds for all $y\in R^m$ satisfying $y\ge 0$.

We claim that the last inequality implies that 
\[
  Ax_0 \le 0. 
\]
For if the $i$-th coordinate $(Ax_0)_i$ is positive, then taking 
$y=\lambda e_i$ for $\lambda>0$ we get
\[
  \lambda (Ax_0)_i < \alpha
\]
which does not hold for large $\lambda$. Thus $(1)$ has a solution.
\end{proof}

%%%%%%%%%%%%%%%
\begin{proof}[Proof using Fourier-Motzkin elimination]
First we suppose that $(2)$ has a solution. 
\footnote{This part is the same as the proof above.}
If $(1)$ also has a soultion, then
\[
  \langle b,x \rangle = \langle A^T y, x \rangle = \langle y,Ax \rangle \le 0,
\]
because $Ax\le 0$ and $y\ge 0$. This contradicts with the assumption that
$\langle b,x \rangle >0$, thus implies that $(1)$ does not have a solution.

Next we suppose that $(2)$ does not have a solution. 
\footnote{Adapted from
\url{http://math.stackexchange.com/questions/225459/fakas-lemma-proof}.
}
First note that problem $(2)$ is equivalent to problem $D y\le d$, where
\[
  D=
  \begin{pmatrix}
    A^T \\
    -A^T \\
    -I
  \end{pmatrix},
  \qquad
  d=
  \begin{pmatrix}
    b \\
    -b \\
    0
  \end{pmatrix}.
\]
We can now apply Fourier-Motzkin Elimination (FME) 
\footnote{see Schrijver, Theory of Linear and Integer Programming, pp.155-157}
on the system $D y\le b$,
removing all variables $y_1,y_2,\cdots,y_m$ in order. Define $U_i$ to be the
matrix used to remove variable $y_i$ from the system of inequalities, i.e.,
to find a matrix $U_1$ to make the first column of $U_1 D$ all zero, 
a matrix $U_2$ to make the first and second column of $U_2 U_1 D$ all zero, 
a matrix $U_3$ to make the first three columns of $U_3 U_2 U_1 D$ all zero, etc.,
a matrix $U_n$ to make all the columns of $U_n U_{n-1}\dots U_2 U_1 D$ all zero.
The standard procedure can be done with $U_i\ge 0$, i.e., all elements of it 
being nonnegative. We denote $U=U_n U_{n-1}\dots U_2 U_1$, hence $U D=0$ and
$U\ge 0$ since $U_i\ge 0$ for all $i$.

Now since the system of inequalities $D y\le d$ has no solution, there must
exist a row vector $u$ of $U$ such that $u^T D=0$ and $u^T d<0$. 
\footnote{Since $u^T D y=0^T y=0\le u^T d$, and there will only be conflict iff
$u^T d<0$.}
Let 
\[
  u = 
    \begin{pmatrix}
      p \\ 
      q \\
      r
    \end{pmatrix},
\]
where $p,q,r\ge 0$ are vectors of order $m$, we then have
\[
  u^T D =
  \begin{pmatrix}
    p^T & q^T & r^T
  \end{pmatrix}
  \begin{pmatrix}
    A^T \\
    -A^T \\
    -I
  \end{pmatrix}
  =0
\]
and 
\[
  u^T d =
  \begin{pmatrix}
    p^T & q^T & r^T
  \end{pmatrix}
  \begin{pmatrix}
    b \\
    -b \\
    0  
  \end{pmatrix}
  <0.
\]
From the first condiction we get
\[
  (q^T-p^T) A^T = -r^T \le 0,
\]
hence 
\[
  A (q-p) \le 0.
\]
And from the second condition we get
\[
  (q^T - p^T) b >0,
\]
hence
\[
  b^T (q-p)>0.
\]
Let $x=q-p$, it satisfies $A x\le 0$ and $b^T x>0$, i.e. $x$ is a solution of
problem $(1)$.
\end{proof}
